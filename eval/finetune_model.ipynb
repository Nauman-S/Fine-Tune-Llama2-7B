{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2da7a35",
   "metadata": {
    "id": "b2da7a35"
   },
   "source": [
    "# AlpacaEval 2 Benchmark Evaluation\n",
    "\n",
    "##  Purpose\n",
    "This notebook implements runs on the fine-tuned model. This is a critical component of the evaluation framework required by [Assignment 7](../../tasks/Assignment7.md) to run the model weights on different inputs.\n",
    "\n",
    "## Evaluation Context\n",
    "Following the complete fine-tuning pipeline:\n",
    "\n",
    "1. **[Baseline Model Evaluation](baseline_model.ipynb)** - Established baseline performance metrics\n",
    "2. **[Fine-tuning Process](finetune_model.ipynb)** - LoRA fine-tuning on Dolly-15K dataset  \n",
    "3. **This Notebook** - AlpacaEval 2 evaluation of fine-tuned model\n",
    "4. **MT-Bench Evaluation** - Multi-turn dialogue assessment (separate notebook)\n",
    "\n",
    "## AlpacaEval 2 Framework\n",
    "- **Repository**: https://github.com/tatsu-lab/alpaca_eval\n",
    "- **Purpose**: Standardized evaluation dataset for instruction-following models\n",
    "- **Dataset**: 805 diverse instruction-following examples\n",
    "- **Method**: Generate responses with our model, then use automated judge (GPT-4) to compare against reference responses\n",
    "- **Metrics**: Win rate (how often our model's response is preferred over reference)\n",
    "\n",
    "## Expected Improvements\n",
    "After LoRA fine-tuning on Dolly-15K, we expect to see:\n",
    "- **Higher win rates** against reference responses\n",
    "- **Better instruction following** quality\n",
    "- **More helpful and coherent** responses\n",
    "- **Improved formatting** and structure\n",
    "\n",
    "## Technical Implementation\n",
    "- **Model**: Fine-tuned LLaMA-2-7B with LoRA adapters\n",
    "- **Process**:\n",
    "  1. Load our fine-tuned model\n",
    "  2. Generate responses to AlpacaEval dataset instructions\n",
    "  3. Use GPT-4 as automated judge to compare our responses vs. reference responses\n",
    "  4. Calculate win rate (percentage of times our response is preferred)\n",
    "- **Comparison**: Fine-tuned vs. baseline model win rates\n",
    "\n",
    "## Workflow\n",
    "1. **Load fine-tuned model** from saved LoRA adapters\n",
    "2. **Load AlpacaEval dataset** (805 instruction examples)\n",
    "3. **Generate responses** using our fine-tuned model\n",
    "4. **Run automated evaluation** using GPT-4 judge to compare against reference responses\n",
    "5. **Calculate win rate** and compare with baseline model results\n",
    "6. **Document metrics** for final report\n",
    "\n",
    "## Success Criteria\n",
    "- **Higher win rate** than baseline model on AlpacaEval dataset\n",
    "- **Measurable improvement** in instruction-following quality\n",
    "- **Consistent performance** across different instruction types\n",
    "- **Clear evidence** of fine-tuning effectiveness\n",
    "\n",
    "---\n",
    "**Note**: This evaluation is essential for demonstrating that our LoRA fine-tuning approach successfully improves the model's instruction-following capabilities on the Dolly-15K dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/tatsu-lab/alpaca_eval.git"
   ],
   "metadata": {
    "id": "m4oFyFY-lsKH"
   },
   "id": "m4oFyFY-lsKH",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U transformers peft bitsandbytes"
   ],
   "metadata": {
    "id": "h07iUu5HsCGw"
   },
   "id": "h07iUu5HsCGw",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import hf_hub_url\n",
    "from peft import PeftModel\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "from google.colab import drive"
   ],
   "metadata": {
    "id": "sXsuyzLGl0R4"
   },
   "id": "sXsuyzLGl0R4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "metadata": {
    "id": "_1Ng1KEPmwPX"
   },
   "id": "_1Ng1KEPmwPX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ],
   "metadata": {
    "id": "uFAb7_j0sd3g"
   },
   "id": "uFAb7_j0sd3g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Load the fine-tuned model\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "adapter_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/results/final_lora_adapter\"\n",
    "print(f\"Loading Fine-Tuned LoRA adapter from: {adapter_path}...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(model)\n",
    "\n"
   ],
   "metadata": {
    "id": "PU3TQaQHnP2f"
   },
   "id": "PU3TQaQHnP2f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Do a simple run with the fine tuned model\n",
    "prompt = \"What is the capital of France\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_text = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "id": "4deZdJ6xoCBJ"
   },
   "id": "4deZdJ6xoCBJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_dir = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs\"\n",
    "baseline_parquet_path = os.path.join(output_dir, \"baseline_model_outputs.parquet\")\n",
    "\n",
    "eval_set_with_baseline = pl.read_parquet(baseline_parquet_path)\n",
    "\n",
    "eval_set_with_baseline.head(10)"
   ],
   "metadata": {
    "id": "VBGPqxzQoiFa"
   },
   "id": "VBGPqxzQoiFa",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "instructions = eval_set_with_baseline.get_column(\"instruction\").to_list()\n",
    "finetuned_outputs = []\n",
    "BATCH_SIZE = 1\n",
    "print(f\"\\nGenerating {len(instructions)} outputs from fine-tuned model in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "for i in tqdm(range(0, len(instructions), BATCH_SIZE)):\n",
    "    batch_instructions = instructions[i : i + BATCH_SIZE]\n",
    "    prompts = [PROMPT_TEMPLATE.format(instruction=inst) for inst in batch_instructions]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output_tokens = generate_ids[:, inputs.input_ids.shape[1]:]\n",
    "    batch_outputs = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "    finetuned_outputs.extend(batch_outputs)\n",
    "\n",
    "eval_set_complete = eval_set_with_baseline.with_columns(\n",
    "    pl.Series(\"finetuned_output\", finetuned_outputs)\n",
    ")\n",
    "\n",
    "eval_set_complete.head()"
   ],
   "metadata": {
    "id": "HzF5R7dFtD9g"
   },
   "id": "HzF5R7dFtD9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "random_row_with_output = eval_set_complete.sample(n=1)\n",
    "print(f\"DATASET: {random_row_with_output['dataset'][0]}\")\n",
    "print(f\"INSTRUCTION: {random_row_with_output['instruction'][0]}\")\n",
    "print(f\"GENERATOR: {random_row_with_output['generator'][0]}\")\n",
    "print(f\"OUTPUT:\\n {random_row_with_output['output'][0]}\")\n",
    "\n",
    "print(f\"BASELINE:\\n {random_row_with_output['baseline_output'][0]}\")\n",
    "\n",
    "print(f\"FINE-TUNED:\\n {random_row_with_output['finetuned_output'][0]}\")"
   ],
   "metadata": {
    "id": "0K7U96IXytaz"
   },
   "id": "0K7U96IXytaz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output_dir = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs\"\n",
    "combined_parquet_path = os.path.join(output_dir, \"eval_outputs_combined.parquet\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving combined DataFrame (baseline + fine-tuned) to: {combined_parquet_path}\")\n",
    "eval_set_complete.write_parquet(combined_parquet_path)\n",
    "!ls -lh \"{combined_parquet_path}\""
   ],
   "metadata": {
    "id": "XRa38L9qz_yr"
   },
   "id": "XRa38L9qz_yr",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}