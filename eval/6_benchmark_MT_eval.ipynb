{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414f5410",
   "metadata": {
    "id": "414f5410"
   },
   "source": [
    "# MT-Bench Multi-Turn Dialogue Evaluation\n",
    "\n",
    "## Purpose\n",
    "This notebook implements **MT-Bench (FastChat)** evaluation to assess multi-turn dialogue quality of our fine-tuned LLaMA-2-7B model. This is the second critical benchmark required by [Assignment 7](../../tasks/Assignment7.md) to demonstrate the effectiveness of our LoRA fine-tuning approach on conversational capabilities.\n",
    "\n",
    "## Evaluation Context\n",
    "Following the complete fine-tuning pipeline:\n",
    "\n",
    "1. **[Baseline Model Evaluation](2_baseline_model.ipynb)** - Established baseline performance metrics\n",
    "2. **[Fine-tuning Process](3_finetuning.ipynb)** - LoRA fine-tuning on Dolly-15K dataset  \n",
    "3. **[Fine-tuned Model Testing](4_finetune_model.ipynb)** - Inference and testing with fine-tuned model\n",
    "4. **[AlpacaEval 2 Benchmark](5_benchmark_alpaca_eval.ipynb)** - Instruction-following quality assessment\n",
    "5. **This Notebook** - MT-Bench multi-turn dialogue evaluation\n",
    "6. **Final Report** - Comprehensive analysis and results\n",
    "\n",
    "## MT-Bench Framework\n",
    "- **Repository**: https://github.com/lm-sys/FastChat\n",
    "- **Purpose**: Multi-turn dialogue quality evaluation\n",
    "- **Dataset**: 80 multi-turn conversations across 8 categories\n",
    "- **Method**: GPT-4 as judge to evaluate conversation quality, helpfulness, and consistency\n",
    "- **Metrics**: Overall score, category-specific scores, consistency across turns\n",
    "\n",
    "## Expected Improvements\n",
    "After LoRA fine-tuning on Dolly-15K, we expect to see:\n",
    "- **Better conversation flow** - More coherent multi-turn interactions\n",
    "- **Improved consistency** - Maintains context across conversation turns\n",
    "- **Enhanced helpfulness** - More useful and relevant responses in conversations\n",
    "- **Better dialogue structure** - Proper conversation formatting and flow\n",
    "\n",
    "## Technical Implementation\n",
    "- **Model**: Fine-tuned LLaMA-2-7B with LoRA adapters\n",
    "- **Process**:\n",
    "  1. Load our fine-tuned model\n",
    "  2. Run MT-Bench evaluation dataset (80 multi-turn conversations)\n",
    "  3. Use GPT-4 as automated judge to evaluate conversation quality\n",
    "  4. Calculate overall and category-specific scores\n",
    "- **Comparison**: Fine-tuned vs. baseline model performance on multi-turn dialogue\n",
    "\n",
    "## Workflow\n",
    "1. **Load fine-tuned model** from saved LoRA adapters\n",
    "2. **Load MT-Bench dataset** (80 multi-turn conversations)\n",
    "3. **Run evaluation** using MT-Bench framework\n",
    "4. **Generate scores** for overall and category-specific performance\n",
    "5. **Compare results** with baseline model performance\n",
    "6. **Document metrics** for final report\n",
    "\n",
    "## Success Criteria\n",
    "- **Higher overall score** than baseline model on MT-Bench\n",
    "- **Improved category scores** across different conversation types\n",
    "- **Better consistency** in multi-turn conversations\n",
    "- **Clear evidence** of enhanced conversational capabilities\n",
    "\n",
    "## MT-Bench Categories\n",
    "The evaluation covers 8 conversation categories:\n",
    "1. **Writing** - Creative and technical writing tasks\n",
    "2. **Roleplay** - Character and scenario-based conversations\n",
    "3. **Reasoning** - Logical and mathematical reasoning\n",
    "4. **Math** - Mathematical problem solving\n",
    "5. **Coding** - Programming and code-related discussions\n",
    "6. **Extraction** - Information extraction tasks\n",
    "7. **STEM** - Science, technology, engineering, math\n",
    "8. **Humanities** - Social sciences, history, philosophy\n",
    "\n",
    "---\n",
    "**Note**: This evaluation is essential for demonstrating that our LoRA fine-tuning approach successfully improves the model's multi-turn conversational capabilities, complementing the instruction-following improvements shown in AlpacaEval 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "T_3ZyQNlBalG",
   "metadata": {
    "id": "T_3ZyQNlBalG",
    "outputId": "8d39de78-28e9-46da-d6fb-889f194601f9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Cloning into 'FastChat'...\n",
      "remote: Enumerating objects: 8528, done.\u001b[K\n",
      "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
      "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
      "remote: Total 8528 (delta 93), reused 37 (delta 37), pack-reused 8413 (from 3)\u001b[K\n",
      "Receiving objects: 100% (8528/8528), 34.53 MiB | 15.48 MiB/s, done.\n",
      "Resolving deltas: 100% (6482/6482), done.\n",
      "/content/FastChat\n",
      "Obtaining file:///content/FastChat\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (3.13.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.119.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.28.1)\n",
      "Requirement already satisfied: markdown2[all] in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.5.4)\n",
      "Requirement already satisfied: nh3 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.0.2)\n",
      "Requirement already satisfied: prompt_toolkit>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (3.0.52)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.11.10)\n",
      "Requirement already satisfied: pydantic-settings in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.11.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (5.9.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.32.4)\n",
      "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (13.9.4)\n",
      "Requirement already satisfied: shortuuid in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (1.0.13)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.12.0)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.37.0)\n",
      "Requirement already satisfied: accelerate>=0.21 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (1.10.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.17.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (0.2.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (4.57.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (5.29.5)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from fschat==0.2.36) (1.109.1)\n",
      "Collecting anthropic (from fschat==0.2.36)\n",
      "  Downloading anthropic-0.71.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting openai (from fschat==0.2.36)\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting ray (from fschat==0.2.36)\n",
      "  Downloading ray-2.50.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.21->fschat==0.2.36) (25.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.21->fschat==0.2.36) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.21->fschat==0.2.36) (0.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic->fschat==0.2.36) (4.15.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->fschat==0.2.36) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->fschat==0.2.36) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->fschat==0.2.36) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->fschat==0.2.36) (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai->fschat==0.2.36) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit>=3.0.0->fschat==0.2.36) (0.2.14)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->fschat==0.2.36) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->fschat==0.2.36) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->fschat==0.2.36) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.0.0->fschat==0.2.36) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.0.0->fschat==0.2.36) (2.19.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->fschat==0.2.36) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.31.0->fschat==0.2.36) (0.22.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->fschat==0.2.36) (1.22.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->fschat==0.2.36) (0.48.0)\n",
      "Requirement already satisfied: wavedrom in /usr/local/lib/python3.12/dist-packages (from markdown2[all]->fschat==0.2.36) (2.0.3.post3)\n",
      "Requirement already satisfied: latex2mathml in /usr/local/lib/python3.12/dist-packages (from markdown2[all]->fschat==0.2.36) (3.78.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings->fschat==0.2.36) (1.1.1)\n",
      "Collecting click!=8.3.0,>=7.0 (from ray->fschat==0.2.36)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray->fschat==0.2.36) (4.25.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray->fschat==0.2.36) (1.1.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.21->fschat==0.2.36) (1.1.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.36) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->fschat==0.2.36) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->fschat==0.2.36) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray->fschat==0.2.36) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray->fschat==0.2.36) (0.27.1)\n",
      "Requirement already satisfied: svgwrite in /usr/local/lib/python3.12/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from wavedrom->markdown2[all]->fschat==0.2.36) (1.17.0)\n",
      "Downloading anthropic-0.71.0-py3-none-any.whl (355 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.0/355.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.50.1-cp312-cp312-manylinux2014_x86_64.whl (71.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: fschat\n",
      "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fschat: filename=fschat-0.2.36-0.editable-py3-none-any.whl size=15067 sha256=59e06d25c90521cb943f690912b7c3fcddfe4bb2b1cdc404c46f853791d22168\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w9uwmi5t/wheels/56/00/76/f8a7b93ea9e9fc4262a0e8aef19f56d8bdd4bc0cfaa413a96c\n",
      "Successfully built fschat\n",
      "Installing collected packages: click, openai, anthropic, ray, fschat\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.3.0\n",
      "    Uninstalling click-8.3.0:\n",
      "      Successfully uninstalled click-8.3.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.109.1\n",
      "    Uninstalling openai-1.109.1:\n",
      "      Successfully uninstalled openai-1.109.1\n",
      "  Attempting uninstall: fschat\n",
      "    Found existing installation: fschat 0.2.36\n",
      "    Uninstalling fschat-0.2.36:\n",
      "      Successfully uninstalled fschat-0.2.36\n",
      "Successfully installed anthropic-0.71.0 click-8.2.1 fschat-0.2.36 openai-0.28.1 ray-2.50.1\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/lm-sys/FastChat.git\n",
    "!git clone https://github.com/lm-sys/FastChat.git\n",
    "%cd FastChat\n",
    "!pip install -e \".[model_worker,llm_judge]\"\n",
    "!pip install -U transformers peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fOfpU64YByLO",
   "metadata": {
    "id": "fOfpU64YByLO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import os\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ViRTxH0KB93-",
   "metadata": {
    "id": "ViRTxH0KB93-",
    "outputId": "eb61d04c-71f1-40b7-8561-c7468f17c6c8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ],
   "metadata": {
    "id": "WuPY27_YHR4j",
    "outputId": "1d461fe5-d90b-4937-b100-f98a96f44f35"
   },
   "id": "WuPY27_YHR4j",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddde8b0550a245568a582c50ba32796c"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "IMqkRjKkCf0o",
   "metadata": {
    "id": "IMqkRjKkCf0o",
    "outputId": "70aad71d-f0ea-4d38-cdb7-28cc2084606b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd9dc1a78ccd4f1b95efa545d9e38c3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f459d816208d4bb2bdb7268603bdbdde"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b2de0fc249842ffaf22c37b187ab342"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e68658ae6652424aa2dd8cf36bc18236"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b847bd1d418b4b538b2721d4c5eef1c4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa5b2e13009649ebb8f27c3ca9e3e4e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "162dac1460994e8eb0c2386e2af5057a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Fine-Tuned LoRA adapter from: /content/drive/MyDrive/LLaMA2-Dolly-Training/results/final_lora_adapter...\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Load the fine-tuned model\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "adapter_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/results/final_lora_adapter\"\n",
    "print(f\"Loading Fine-Tuned LoRA adapter from: {adapter_path}...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "merged_model_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/models/Llama-2-7b-hf-dolly-merged\"\n",
    "model = model.merge_and_unload()\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(model)"
   ],
   "metadata": {
    "id": "A2R1zE0oIGF-",
    "outputId": "2eaf1059-b0ef-4957-d286-5918e34b3210"
   },
   "id": "A2R1zE0oIGF-",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_dir = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs\"\n",
    "base_model_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/models/Llama-2-7b-hf\"\n",
    "merged_model_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/models/Llama-2-7b-hf-dolly-merged\"\n",
    "\n",
    "model_id_finetuned = \"llama-2-7b-dolly-qlora\"\n",
    "default_answer_file_finetuned = f\"/content/FastChat/fastchat/llm_judge/data/mt_bench/model_answer/{model_id_finetuned}.jsonl\"\n",
    "gdrive_answer_file_finetuned = f\"{output_dir}/{model_id_finetuned}_mt_bench_answers.jsonl\"\n",
    "\n",
    "model_id_baseline = \"llama-2-7b-hf-baseline\"\n",
    "default_answer_file_baseline = f\"/content/FastChat/fastchat/llm_judge/data/mt_bench/model_answer/{model_id_baseline}.jsonl\"\n",
    "gdrive_answer_file_baseline = f\"{output_dir}/{model_id_baseline}_mt_bench_answers.jsonl\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "fastchat_judge_dir = \"/content/FastChat/fastchat/llm_judge\"\n",
    "os.chdir(fastchat_judge_dir)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "13slybsXKwo7"
   },
   "id": "13slybsXKwo7",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!echo \"Generating MT-Bench answers for Fine-Tuned Model (${model_id_finetuned})...\"\n",
    "!python3 gen_model_answer.py --model-path \"{merged_model_path}\" --max-new-token 2048 --model-id \"{model_id_finetuned}\" --answer-file \"{gdrive_answer_file_finetuned}\"\n",
    "!echo \"✓ Fine-tuned answers generated to default path: {default_answer_file_finetuned}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "UOCk8FZwO-jm",
    "outputId": "6d7fa5dd-c803-4688-b892-156c8ac34135"
   },
   "id": "UOCk8FZwO-jm",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating MT-Bench answers for Fine-Tuned Model (-2-7b-dolly-qlora)...\n",
      "2025-10-19 20:02:12.059426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760904132.081009   10128 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760904132.088307   10128 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760904132.106207   10128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760904132.106244   10128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760904132.106247   10128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760904132.106249   10128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Output to /content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/llama-2-7b-dolly-qlora_mt_bench_answers.jsonl\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "  0% 0/80 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "  5% 4/80 [12:45<3:46:55, 179.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100% 80/80 [4:46:16<00:00, 214.71s/it]\n",
      "✓ Fine-tuned answers generated to default path: /content/FastChat/fastchat/llm_judge/data/mt_bench/model_answer/llama-2-7b-dolly-qlora.jsonl\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nGenerating MT-Bench answers for Baseline Model...\")\n",
    "!python3 gen_model_answer.py \\\n",
    "    --model-path \"{base_model_path}\" \\\n",
    "    --model-id \"{model_id_baseline}\" \\\n",
    "    --answer-file \"{gdrive_answer_file_baseline}\" \\\n",
    "    --max-new-token 2048"
   ],
   "metadata": {
    "id": "SqspyuFHAgVe",
    "outputId": "0b0a3fa8-2607-4dfc-b2d9-f8950160680f"
   },
   "id": "SqspyuFHAgVe",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Generating MT-Bench answers for Baseline Model...\n",
      "2025-10-20 01:37:26.255957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760924246.277640   96600 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760924246.284250   96600 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760924246.300770   96600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760924246.300794   96600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760924246.300797   96600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760924246.300800   96600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Output to /content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/llama-2-7b-hf-baseline_mt_bench_answers.jsonl\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100% 3/3 [03:37<00:00, 72.57s/it]\n",
      "  0% 0/80 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "100% 80/80 [2:31:46<00:00, 113.83s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(gdrive_answer_file_baseline)\n",
    "print(gdrive_answer_file_finetuned)"
   ],
   "metadata": {
    "id": "NBFXodRdCeM8",
    "outputId": "0e412bbb-2dad-4723-8fae-9d061bde6e27"
   },
   "id": "NBFXodRdCeM8",
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/llama-2-7b-hf-baseline_mt_bench_answers.jsonl\n",
      "/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/llama-2-7b-dolly-qlora_mt_bench_answers.jsonl\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}