{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b1716d",
   "metadata": {
    "id": "56b1716d"
   },
   "source": [
    "# Baseline Model Evaluation\n",
    "\n",
    "## Purpose\n",
    "This notebook benchmarks the **baseline LLaMA-2-7B model** before fine-tuning to establish performance baselines for comparison with the fine-tuned model.\n",
    "\n",
    "## Evaluation Framework\n",
    "Following the requirements from [Assignment 7](../../tasks/Assignment7.md), we run the baseline model using:\n",
    "\n",
    "### 1. **AlpacaEval 2**\n",
    "- **Purpose**: Instruction-following quality assessment\n",
    "- **Repository**: https://github.com/tatsu-lab/alpaca_eval\n",
    "- **Metrics**: Win rate against reference model, response quality\n",
    "\n",
    "### 2. **MT-Bench (FastChat)**\n",
    "- **Purpose**: Multi-turn dialogue quality evaluation  \n",
    "- **Repository**: https://github.com/lm-sys/FastChat\n",
    "- **Metrics**: Multi-turn conversation capability, consistency\n",
    "\n",
    "## Workflow\n",
    "1. **Load baseline model**: `meta-llama/Llama-2-7b-hf`\n",
    "2. **Run AlpacaEval 2**: Generate responses and calculate win rates\n",
    "3. **Run MT-Bench**: Evaluate multi-turn dialogue performance\n",
    "4. **Document results**: Baseline metrics for comparison with fine-tuned model\n",
    "\n",
    "## Expected Outcomes\n",
    "- Establish baseline performance metrics\n",
    "- Identify areas where fine-tuning should improve\n",
    "- Provide comparison baseline for fine-tuned model evaluation\n",
    "- Demonstrate clear performance gaps that fine-tuning aims to address\n",
    "\n",
    "---\n",
    "**Note**: This baseline evaluation is crucial for demonstrating the effectiveness of our LoRA fine-tuning approach on the Dolly-15K dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "XWPM-NTaY1E1",
   "metadata": {
    "id": "XWPM-NTaY1E1",
    "outputId": "5f059f35-558d-4542-ebd8-ce933b684fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "VZRqko0lY8fB",
   "metadata": {
    "id": "VZRqko0lY8fB",
    "outputId": "ae454207-decb-4856-f1a1-810fd306790c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5b07cfede74482a704fa165decdf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached model in Google Drive, Loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21de2a86fb74f5493677ba0c57290c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model meta-llama/Llama-2-7b-hf loaded\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)\n",
    "\n",
    "#Load the model\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "cache_path = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/models/Llama-2-7b-hf\"\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(f\"Found cached model in Google Drive, Loading...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cache_path,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        local_files_only=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cache_path)\n",
    "    print(f\"✓ Model {model_id} loaded\")\n",
    "else:\n",
    "    print(f\"Downloading {model_id} for the first time...\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    model.save_pretrained(cache_path)\n",
    "    tokenizer.save_pretrained(cache_path)\n",
    "    print(f\"✓ Cached to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cF3-osnCErav",
   "metadata": {
    "id": "cF3-osnCErav",
    "outputId": "d3723675-8155-4caf-8515-2f24fdbd5fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "How many regions are in France?\n",
      "What is the currency of France?\n",
      "What is the largest city in France?\n",
      "What is the official language of France?\n",
      "What is the religion of France?\n",
      "What is the population of France\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "generate_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "generated_text = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "XvSlYQPSJqC0",
   "metadata": {
    "id": "XvSlYQPSJqC0",
    "outputId": "e39c2a81-f76a-46d4-833a-1967cffe181a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from: https://huggingface.co/datasets/tatsu-lab/alpaca_eval/resolve/main/alpaca_eval.json\n",
      "✓ Loaded 805 instructions\n",
      "✓ Sampled 10 instructions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dataset</th><th>instruction</th><th>output</th><th>generator</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;koala&quot;</td><td>&quot;in billiards what happens if o…</td><td>&quot;If every striped ball is pocke…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;i assume you are familiar with…</td><td>&quot;Estimates and Error Margins:\n",
       "•…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;selfinstruct&quot;</td><td>&quot;Give students tips on how to k…</td><td>&quot;1. Practice your presentation …</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;helpful_base&quot;</td><td>&quot;what is the name of chris tuck…</td><td>&quot;Chris Tucker&#x27;s first movie was…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;Please summarise in point form…</td><td>&quot;1. Decline in agricultural pro…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;helpful_base&quot;</td><td>&quot;I&#x27;m trying to teach myself to …</td><td>&quot;Sure! Here are a few tips to h…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;cost of fuel for a 14 mile jou…</td><td>&quot;£3.75&quot;</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;Explain me the Finite Elemente…</td><td>&quot;The Finite Element Method (FEM…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;oasst&quot;</td><td>&quot;How can I use software defined…</td><td>&quot;To detect and locate a drone f…</td><td>&quot;text_davinci_003&quot;</td></tr><tr><td>&quot;selfinstruct&quot;</td><td>&quot;You are given a tweet and you …</td><td>&quot;Offensive&quot;</td><td>&quot;text_davinci_003&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌──────────────┬────────────────────────────────┬───────────────────────────────┬──────────────────┐\n",
       "│ dataset      ┆ instruction                    ┆ output                        ┆ generator        │\n",
       "│ ---          ┆ ---                            ┆ ---                           ┆ ---              │\n",
       "│ str          ┆ str                            ┆ str                           ┆ str              │\n",
       "╞══════════════╪════════════════════════════════╪═══════════════════════════════╪══════════════════╡\n",
       "│ koala        ┆ in billiards what happens if   ┆ If every striped ball is      ┆ text_davinci_003 │\n",
       "│              ┆ o…                             ┆ pocke…                        ┆                  │\n",
       "│ koala        ┆ i assume you are familiar      ┆ Estimates and Error Margins:  ┆ text_davinci_003 │\n",
       "│              ┆ with…                          ┆ •…                            ┆                  │\n",
       "│ selfinstruct ┆ Give students tips on how to   ┆ 1. Practice your presentation ┆ text_davinci_003 │\n",
       "│              ┆ k…                             ┆ …                             ┆                  │\n",
       "│ helpful_base ┆ what is the name of chris      ┆ Chris Tucker's first movie    ┆ text_davinci_003 │\n",
       "│              ┆ tuck…                          ┆ was…                          ┆                  │\n",
       "│ koala        ┆ Please summarise in point      ┆ 1. Decline in agricultural    ┆ text_davinci_003 │\n",
       "│              ┆ form…                          ┆ pro…                          ┆                  │\n",
       "│ helpful_base ┆ I'm trying to teach myself to  ┆ Sure! Here are a few tips to  ┆ text_davinci_003 │\n",
       "│              ┆ …                              ┆ h…                            ┆                  │\n",
       "│ koala        ┆ cost of fuel for a 14 mile     ┆ £3.75                         ┆ text_davinci_003 │\n",
       "│              ┆ jou…                           ┆                               ┆                  │\n",
       "│ koala        ┆ Explain me the Finite          ┆ The Finite Element Method     ┆ text_davinci_003 │\n",
       "│              ┆ Elemente…                      ┆ (FEM…                         ┆                  │\n",
       "│ oasst        ┆ How can I use software         ┆ To detect and locate a drone  ┆ text_davinci_003 │\n",
       "│              ┆ defined…                       ┆ f…                            ┆                  │\n",
       "│ selfinstruct ┆ You are given a tweet and you  ┆ Offensive                     ┆ text_davinci_003 │\n",
       "│              ┆ …                              ┆                               ┆                  │\n",
       "└──────────────┴────────────────────────────────┴───────────────────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "EVAL_MODE = \"sample\"\n",
    "json_url = hf_hub_url(\n",
    "    repo_id=\"tatsu-lab/alpaca_eval\",\n",
    "    filename=\"alpaca_eval.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Downloading data from: {json_url}\")\n",
    "\n",
    "df_pandas = pd.read_json(json_url)\n",
    "eval_set = pl.from_pandas(df_pandas)\n",
    "\n",
    "print(f\"✓ Loaded {eval_set.height} instructions\")\n",
    "\n",
    "if EVAL_MODE == \"sample\":\n",
    "    eval_set = eval_set.sample(n=10, seed=42)\n",
    "    print(f\"✓ Sampled {eval_set.height} instructions\")\n",
    "\n",
    "eval_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd1rC9PePag0",
   "metadata": {
    "id": "dd1rC9PePag0",
    "outputId": "bf487669-df86-4511-c726-71bc544a3ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: koala\n",
      "INSTRUCTION: Please summarise in point form \"Challenges for\n",
      "African Agriculture\" by Jean-Claude Devèze\n",
      "OUTPUT: 1. Decline in agricultural productivity due to overworked soils \n",
      "2. Outdated agricultural practices \n",
      "3. Poor access to markets \n",
      "4. Lack of agricultural diversification \n",
      "5. Poor access to inputs such as fertilizer and modern farming techniques \n",
      "6. Poor infrastructure and lack of transport networks \n",
      "7. Low levels of investment in agricultural research and development \n",
      "8. Natural disasters such as drought and floods \n",
      "9. Poor access to credit and financial services\n",
      "GENERATOR: text_davinci_003\n"
     ]
    }
   ],
   "source": [
    "random_row = eval_set.sample(n=1)\n",
    "print(f\"DATASET: {random_row['dataset'][0]}\")\n",
    "print(f\"INSTRUCTION: {random_row['instruction'][0]}\")\n",
    "print(f\"OUTPUT: {random_row['output'][0]}\")\n",
    "print(f\"GENERATOR: {random_row['generator'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "pN1Vm9h3o6_m",
   "metadata": {
    "id": "pN1Vm9h3o6_m"
   },
   "outputs": [],
   "source": [
    "def generate_response(instruction, reference_output, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate response with adaptive max_tokens based on reference length.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    reference_tokens = len(tokenizer.encode(reference_output))\n",
    "    max_new_tokens = min(max(reference_tokens * 2, 256), 2048)\n",
    "\n",
    "    print(f\"Reference: {reference_tokens} tokens → Using max: {max_new_tokens} tokens\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in generated_text:\n",
    "        response = generated_text.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "OgzxgCwREqZ7",
   "metadata": {
    "id": "OgzxgCwREqZ7"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "qyFnE6O1ZXxJ",
   "metadata": {
    "id": "qyFnE6O1ZXxJ",
    "outputId": "0663dadf-dc43-4799-a8db-ec7dda43495b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation for 10 instructions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a2add8fa824e1e886bffde26ee9484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: 33 tokens → Using max: 256 tokens\n",
      "Reference: 282 tokens → Using max: 564 tokens\n",
      "Reference: 165 tokens → Using max: 330 tokens\n",
      "Reference: 22 tokens → Using max: 256 tokens\n",
      "Reference: 122 tokens → Using max: 256 tokens\n",
      "Reference: 125 tokens → Using max: 256 tokens\n",
      "Reference: 6 tokens → Using max: 256 tokens\n",
      "Reference: 111 tokens → Using max: 256 tokens\n",
      "Reference: 110 tokens → Using max: 256 tokens\n",
      "Reference: 3 tokens → Using max: 256 tokens\n",
      "✓ All responses generated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dataset</th><th>instruction</th><th>output</th><th>generator</th><th>baseline_output</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;koala&quot;</td><td>&quot;in billiards what happens if o…</td><td>&quot;If every striped ball is pocke…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;**f**:\n",
       "\n",
       "```\n",
       "(6)\n",
       "```\n",
       "\n",
       "### Instr…</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;i assume you are familiar with…</td><td>&quot;Estimates and Error Margins:\n",
       "•…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;&lt;img src=&quot;https://i.imgur.com/…</td></tr><tr><td>&quot;selfinstruct&quot;</td><td>&quot;Give students tips on how to k…</td><td>&quot;1. Practice your presentation …</td><td>&quot;text_davinci_003&quot;</td><td>&quot;1. Relax\n",
       "\n",
       "2. Breathe\n",
       "\n",
       "3. Talk …</td></tr><tr><td>&quot;helpful_base&quot;</td><td>&quot;what is the name of chris tuck…</td><td>&quot;Chris Tucker&#x27;s first movie was…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;[The Fifth Element](https://ww…</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;Please summarise in point form…</td><td>&quot;1. Decline in agricultural pro…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;In the article, Devèze summari…</td></tr><tr><td>&quot;helpful_base&quot;</td><td>&quot;I&#x27;m trying to teach myself to …</td><td>&quot;Sure! Here are a few tips to h…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;### Instruction:&quot;</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;cost of fuel for a 14 mile jou…</td><td>&quot;£3.75&quot;</td><td>&quot;text_davinci_003&quot;</td><td>&quot;cost of fuel for a 14 mile jou…</td></tr><tr><td>&quot;koala&quot;</td><td>&quot;Explain me the Finite Elemente…</td><td>&quot;The Finite Element Method (FEM…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;The finite element method (FEM…</td></tr><tr><td>&quot;oasst&quot;</td><td>&quot;How can I use software defined…</td><td>&quot;To detect and locate a drone f…</td><td>&quot;text_davinci_003&quot;</td><td>&quot;The first thing you need to do…</td></tr><tr><td>&quot;selfinstruct&quot;</td><td>&quot;You are given a tweet and you …</td><td>&quot;Offensive&quot;</td><td>&quot;text_davinci_003&quot;</td><td>&quot;Offensive.\n",
       "\n",
       "### Instruction:\n",
       "\n",
       "…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌──────────────┬─────────────────────┬─────────────────────┬──────────────────┬────────────────────┐\n",
       "│ dataset      ┆ instruction         ┆ output              ┆ generator        ┆ baseline_output    │\n",
       "│ ---          ┆ ---                 ┆ ---                 ┆ ---              ┆ ---                │\n",
       "│ str          ┆ str                 ┆ str                 ┆ str              ┆ str                │\n",
       "╞══════════════╪═════════════════════╪═════════════════════╪══════════════════╪════════════════════╡\n",
       "│ koala        ┆ in billiards what   ┆ If every striped    ┆ text_davinci_003 ┆ **f**:             │\n",
       "│              ┆ happens if o…       ┆ ball is pocke…      ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ ```                │\n",
       "│              ┆                     ┆                     ┆                  ┆ (6)                │\n",
       "│              ┆                     ┆                     ┆                  ┆ ```                │\n",
       "│              ┆                     ┆                     ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ ### Instr…         │\n",
       "│ koala        ┆ i assume you are    ┆ Estimates and Error ┆ text_davinci_003 ┆ <img src=\"https:// │\n",
       "│              ┆ familiar with…      ┆ Margins:            ┆                  ┆ i.imgur.com/…      │\n",
       "│              ┆                     ┆ •…                  ┆                  ┆                    │\n",
       "│ selfinstruct ┆ Give students tips  ┆ 1. Practice your    ┆ text_davinci_003 ┆ 1. Relax           │\n",
       "│              ┆ on how to k…        ┆ presentation …      ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ 2. Breathe         │\n",
       "│              ┆                     ┆                     ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ 3. Talk …          │\n",
       "│ helpful_base ┆ what is the name of ┆ Chris Tucker's      ┆ text_davinci_003 ┆ [The Fifth Element │\n",
       "│              ┆ chris tuck…         ┆ first movie was…    ┆                  ┆ ](https://ww…      │\n",
       "│ koala        ┆ Please summarise in ┆ 1. Decline in       ┆ text_davinci_003 ┆ In the article,    │\n",
       "│              ┆ point form…         ┆ agricultural pro…   ┆                  ┆ Devèze summari…    │\n",
       "│ helpful_base ┆ I'm trying to teach ┆ Sure! Here are a    ┆ text_davinci_003 ┆ ### Instruction:   │\n",
       "│              ┆ myself to …         ┆ few tips to h…      ┆                  ┆                    │\n",
       "│ koala        ┆ cost of fuel for a  ┆ £3.75               ┆ text_davinci_003 ┆ cost of fuel for a │\n",
       "│              ┆ 14 mile jou…        ┆                     ┆                  ┆ 14 mile jou…       │\n",
       "│ koala        ┆ Explain me the      ┆ The Finite Element  ┆ text_davinci_003 ┆ The finite element │\n",
       "│              ┆ Finite Elemente…    ┆ Method (FEM…        ┆                  ┆ method (FEM…       │\n",
       "│ oasst        ┆ How can I use       ┆ To detect and       ┆ text_davinci_003 ┆ The first thing    │\n",
       "│              ┆ software defined…   ┆ locate a drone f…   ┆                  ┆ you need to do…    │\n",
       "│ selfinstruct ┆ You are given a     ┆ Offensive           ┆ text_davinci_003 ┆ Offensive.         │\n",
       "│              ┆ tweet and you …     ┆                     ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ ### Instruction:   │\n",
       "│              ┆                     ┆                     ┆                  ┆                    │\n",
       "│              ┆                     ┆                     ┆                  ┆ …                  │\n",
       "└──────────────┴─────────────────────┴─────────────────────┴──────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Starting generation for {eval_set.height} instructions...\")\n",
    "\n",
    "baseline_outputs = []\n",
    "\n",
    "for row in tqdm(eval_set.iter_rows(named=True), total=eval_set.height):\n",
    "    response = generate_response(\n",
    "        instruction=row['instruction'],\n",
    "        reference_output=row['output'],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    baseline_outputs.append(response)\n",
    "\n",
    "print(\"✓ All responses generated.\")\n",
    "\n",
    "eval_set_with_outputs = eval_set.with_columns(\n",
    "    pl.Series(\"baseline_output\", baseline_outputs)\n",
    ")\n",
    "\n",
    "eval_set_with_outputs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "xxUZahF_cPYZ",
   "metadata": {
    "id": "xxUZahF_cPYZ",
    "outputId": "09048fae-a12e-46ce-8642-b735a8013ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: selfinstruct\n",
      "INSTRUCTION: Give students tips on how to keep their nerves under control during class presentations.\n",
      "GENERATOR: text_davinci_003\n",
      "OUTPUT:\n",
      " 1. Practice your presentation beforehand to build confidence and reduce anxiety.\n",
      "2. Make sure you are well-rested and have eaten something before your presentation.\n",
      "3. Take deep breaths to help calm your mind and body.\n",
      "4. Visualize yourself delivering the presentation with confidence.\n",
      "5. Break down your presentation into smaller chunks to make it easier to manage.\n",
      "6. Remind yourself that your audience wants you to succeed.\n",
      "7. Focus on the message you are delivering, not on yourself.\n",
      "8. Remember to speak slowly and clearly to keep your nerves in check.\n",
      "9. If you make a mistake, don't worry about it - just move on.\n",
      "10. Reward yourself after a successful presentation.\n",
      "BASELINE:\n",
      " 1. Relax\n",
      "\n",
      "2. Breathe\n",
      "\n",
      "3. Talk slowly\n",
      "\n",
      "4. Speak clearly\n",
      "\n",
      "5. Be confident\n",
      "\n",
      "6. Practice beforehand\n",
      "\n",
      "7. Sm\n"
     ]
    }
   ],
   "source": [
    "random_row_with_output = eval_set_with_outputs.sample(n=1)\n",
    "print(f\"DATASET: {random_row_with_output['dataset'][0]}\")\n",
    "print(f\"INSTRUCTION: {random_row_with_output['instruction'][0]}\")\n",
    "print(f\"GENERATOR: {random_row_with_output['generator'][0]}\")\n",
    "print(f\"OUTPUT:\\n {random_row_with_output['output'][0]}\")\n",
    "\n",
    "print(f\"BASELINE:\\n {random_row_with_output['baseline_output'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pmsb0taBfNOZ",
   "metadata": {
    "id": "Pmsb0taBfNOZ",
    "outputId": "85e8b032-2d44-4bf1-d931-878152958ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 outputs in batches of 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e166342331e148f4b616fe4069a8b4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 20689 has 14.74 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 22.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3843216428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     ).to(model.device)\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         generate_ids = model.generate(\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    291\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         hidden_states, _ = self.self_attn(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 20689 has 14.74 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 22.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Here we batch them to evaluate larger number of items faster\n",
    "# BATCH_SIZE = 1 # Reduce batch size to avoid OOM errors\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# PROMPT_TEMPLATE = \"\"\"### Instruction:\n",
    "# {instruction}\n",
    "\n",
    "# ### Response:\n",
    "# \"\"\"\n",
    "# instructions = eval_set.get_column(\"instruction\").to_list()\n",
    "# all_outputs = []\n",
    "# print(f\"Generating {len(instructions)} outputs in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "# for i in tqdm(range(0, len(instructions), BATCH_SIZE)):\n",
    "#     batch_instructions = instructions[i : i + BATCH_SIZE]\n",
    "#     prompts = [PROMPT_TEMPLATE.format(instruction=inst) for inst in batch_instructions]\n",
    "#     inputs = tokenizer(\n",
    "#         prompts,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=2048\n",
    "#     ).to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         generate_ids = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=512,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             do_sample=True,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             eos_token_id=tokenizer.eos_token_id\n",
    "#         )\n",
    "#     output_tokens = generate_ids[:, inputs.input_ids.shape[1]:]\n",
    "#     batch_outputs = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "#     all_outputs.extend(batch_outputs)\n",
    "\n",
    "\n",
    "# eval_set_with_outputs = eval_set.with_columns(\n",
    "#     pl.Series(\"baseline_output\", all_outputs)\n",
    "# )\n",
    "# print(eval_set_with_outputs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axkvAG8hfTow",
   "metadata": {
    "id": "axkvAG8hfTow",
    "outputId": "f1a5bdb7-5b72-4909-d7c2-7082ba503dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DataFrame to Google Drive at: /content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/baseline_model_outputs.parquet\n",
      "-rw------- 1 root root 9.8K Oct 18 09:10 /content/drive/MyDrive/LLaMA2-Dolly-Training/outputs/baseline_model_outputs.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"/content/drive/MyDrive/LLaMA2-Dolly-Training/outputs\"\n",
    "file_path = os.path.join(output_dir, \"baseline_model_outputs.parquet\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving DataFrame to Google Drive at: {file_path}\")\n",
    "\n",
    "eval_set_with_outputs.write_parquet(file_path)\n",
    "\n",
    "!ls -lh {file_path}\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}